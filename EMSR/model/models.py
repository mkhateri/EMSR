from collections import OrderedDict
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math


import torch
import torch.nn as nn
# from timm.models.layers import DropPath, trunc_normal_, to_3tuple
from torch.distributions.normal import Normal
import torch.nn.functional as nnf
import numpy as np

import torch.nn.functional as F
import math
from typing import Sequence, Type, Tuple, Union, List, Optional, Dict
from torch import Tensor
from einops.layers.torch import Rearrange
from einops import rearrange
from collections import deque
#from transoar.models.backbones.resnet3d import ResNet3D

ndims = 2 # H,W


#from detectron2.modeling import BACKBONE_REGISTRY, Backbone, ShapeSpec
att_dtype = torch.float16


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



def get_norm(name, **kwargs):
    if name.lower() == 'BatchNorm'.lower():
        BatchNorm = getattr(nn, 'BatchNorm%dd' % ndims)
        return BatchNorm(**kwargs)
    elif name.lower() in ['instance', 'InstanceNorm'.lower()]:
        InstanceNorm = getattr(nn, 'InstanceNorm%dd' % ndims)
        return InstanceNorm(**kwargs)
    elif name.lower() == 'None'.lower():
        return nn.Identity()
    else:
        return NotImplementedError


def get_activation(name, **kwargs):
    if name.lower() == 'ReLU'.lower():
        return nn.ReLU()
    elif name.lower() == 'GELU'.lower():
        return nn.GELU()
    elif name.lower() == 'None'.lower():
        return nn.Identity()
    else:
        return NotImplementedError



def downsampler_fn(data, out_size):
    """
    input sahep: B,Ci,Hi,Wi,Di
    output sahep: B,C,H,W,D

    """
    out = nn.functional.interpolate(data,
                                     size=out_size,
                                     mode='trilinear',
                                     align_corners=None,
                                     recompute_scale_factor=None,
                                     #antialias=False
    )
    return out.to(data.get_device())

# this files is adopted from timm.
def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.
    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
    if keep_prob > 0.0 and scale_by_keep:
        random_tensor.div_(keep_prob)
    return x * random_tensor


class timm_DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):
        super(timm_DropPath, self).__init__()
        self.drop_prob = drop_prob
        self.scale_by_keep = scale_by_keep

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)

    def extra_repr(self):
        return f'drop_prob={round(self.drop_prob,3):0.3f}'


def _trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * l - 1, 2 * u - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor

def timm_trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are
    applied while sampling the normal with mean/std applied, therefore a, b args
    should be adjusted to match the range of mean, std args.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    with torch.no_grad():
        return _trunc_normal_(tensor, mean, std, a, b)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)



ndims = 2 # H,W,D

def get_norm(name, **kwargs):
    if name.lower() == 'BatchNorm2d'.lower():
        BatchNorm = getattr(nn, 'BatchNorm%dd' % ndims)
        return BatchNorm(**kwargs)
    elif name.lower() == 'instance':
        InstanceNorm = getattr(nn, 'InstanceNorm%dd' % ndims)
        return InstanceNorm(**kwargs)
    elif name.lower() == 'None'.lower():
        return nn.Identity()
    else:
        return NotImplementedError


def get_activation(name, **kwargs):
    if name.lower() == 'ReLU'.lower():
        return nn.ReLU()
    elif name.lower() == 'GELU'.lower():
        return nn.GELU()
    elif name.lower() == 'None'.lower():
        return nn.Identity()
    else:
        return NotImplementedError




from functools import reduce
def prod_func(Vec):
  return reduce( lambda x, y: x*y, Vec ) #  math.prod()

def downsampler_fn(img, out_size):
    """
    input sahep: B,C,H,W,D
    output sahep: B,C,H,W,D

    """
    return nn.functional.interpolate(img,
                                     size=out_size,
                                     mode='nearest',
                                     align_corners=None,
                                     recompute_scale_factor=None,
                                     #antialias=False
                                     )


class MLP(nn.Module):
    def __init__(self,
                in_feats,
                MLP_type="basic", # scmlp conv basic
                hid_feats=None,
                out_feats=None,
                kernel_size=3,
                act_name="GELU",
                drop=0.,
                bias=False,
            )->None:
        super(MLP, self).__init__()

        out_feats = out_feats or in_feats
        hid_feats = hid_feats or in_feats

        '''
        MLP_type.lower()=="scmlpv4" "scmlpv4":
            self.MLP_scmlp = nn.Sequential(*[
                Rearrange('B h w c -> B c h w'),
                nn.Conv2d(in_feats, out_feats, kernel_size=1, bias=bias),
                nn.BatchNorm2d(out_feats),
                get_activation(act_name),
                nn.Conv2d(out_feats, out_feats, kernel_size=kernel_size,
                    padding=int(kernel_size//2), groups=out_feats, bias=bias),
                nn.BatchNorm2d(out_feats),
                get_activation(act_name),
                #eca_layer(out_feats),   scmlpv3 <<<<<<<<<<<<<<<<<<<
                Rearrange('B c h w -> B h w c'),
                nn.Linear(out_feats, out_feats),
                get_activation(act_name),
                nn.Dropout(drop),
                nn.Linear(out_feats, out_feats),
                nn.Dropout(drop),
            ])
        '''

        # Mori: I perefer using conv (in_feats->out_feats) + 2x Linear (out_feats -> out_feats)
        # rather than conventional  Linear (in_feats -> hid_feats) + Linear (hid_feats -> out_feats)
        # never use squeeze in image2image translation
        if MLP_type.lower()=="scmlp":
            # improved MLP : 3x3conv (spatial) -> eca (channel) -> mlp
            self.net = nn.Sequential(*[
                Rearrange('B h w c -> B c h w'),
                nn.Conv3d(in_feats, out_feats, kernel_size=1, bias=bias),
                nn.BatchNorm3d(out_feats),
                get_activation(act_name),
                Rearrange('B c h w d -> B h w d c'),
                nn.Linear(out_feats, out_feats),
                get_activation(act_name),
                nn.Dropout(drop),
                nn.Linear(out_feats, out_feats),
                get_activation(act_name),
                nn.Dropout(drop),
            ])


        elif MLP_type.lower()=="conv":
            # improved MLP # RVT cvpr2022
            self.net = nn.Sequential(*[
                Rearrange('B h w c -> B c h w'),
                nn.Conv3d(in_feats, hid_feats, kernel_size=1, bias=bias),
                nn.BatchNorm2d(hid_feats),
                get_activation(act_name),
                nn.Dropout(drop),
                nn.Conv3d(hid_feats, hid_feats, kernel_size=kernel_size,
                    padding=int(kernel_size//2), groups=hid_feats, bias=bias),
                nn.BatchNorm2d(hid_feats),
                get_activation(act_name),
                nn.Conv3d(hid_feats, out_feats, kernel_size=1, bias=bias),
                nn.BatchNorm2d(out_feats),
                nn.Dropout(drop),
                Rearrange('B c h w-> B h w c'),
            ])


        elif MLP_type.lower()=="basic":
            self.net = nn.Sequential(*[
                    nn.Linear(in_feats, hid_feats),
                    get_activation(act_name),
                    nn.Dropout(drop),
                    nn.Linear(hid_feats, out_feats),
                    nn.Dropout(drop)
            ])

    def forward(self, x)-> torch.Tensor:
        x = self.net(x)
        return x




class Attention(nn.Module):
    def __init__(self,
                dim,
                num_heads,
                patch_size,
                attention_type = "local",
                qkv_bias=True,
                qk_scale=None,
                attn_drop=0.,
                proj_drop=0.,
            )->None:

        super().__init__()

        if isinstance(patch_size, int):
            patch_size = [patch_size]*ndims
        self.patch_size = patch_size

        self.num_heads = num_heads

        assert dim%num_heads==0, "`dim` must be divisible by `num_heads`"
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5


        # Mori: local need bias, but not true about global one
        if attention_type == "local":
            # define a parameter table of relative position bias
            relative_position_bias_table = nn.Parameter(
                    torch.zeros((2 * self.patch_size[0] - 1) * (2 * self.patch_size[1] - 1) * (2 * self.patch_size[2] - 1),
                                self.num_heads)
                    )  # 2*Ww-1 * 2*Wh-1 * 2*Wd-1, nH

            # get pair-wise relative position index for each token inside the window
            coords_d = torch.arange(self.patch_size[0])
            coords_h = torch.arange(self.patch_size[1])
            coords_w = torch.arange(self.patch_size[2])
            coords = torch.stack(torch.meshgrid(coords_w, coords_h, coords_d, indexing="ij"))  # 3, Ww, Wh, Wd
            coords_flatten = torch.flatten(coords, 1)  # 3, Ww*Wh*Wd
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 3, Ww*Wh*Wd, Ww*Wh*Wd
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Ww*Wh*Wd, Ww*Wh*Wd, 3
            relative_coords[:, :, 0] += self.patch_size[0] - 1  # shift to start from 0
            relative_coords[:, :, 1] += self.patch_size[1] - 1
            relative_coords[:, :, 2] += self.patch_size[2] - 1

            relative_coords[:, :, 0] *= (2 * self.patch_size[1] - 1) * (2 * self.patch_size[2] - 1)
            relative_coords[:, :, 1] *= (2 * self.patch_size[2] - 1)
            relative_position_index = relative_coords.sum(-1)  # Ww*Wh*Wd, Ww*Wh*Wd
            #register_buffer("relative_position_index", relative_position_index)

        self.attention_type = attention_type

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)

        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        #self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        #x_dtype = x.dtype
        #x = x.type(att_dtype)
        B_, N, C = x.size()
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)

        q, k, v = qkv[0], qkv[1], qkv[2]
        q = q * self.scale

        attn = (q @ k.transpose(-2, -1))

        '''
        if self.attention_type == 'local':
            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
                self.patch_size[0]**ndims, self.patch_size[0]**ndims, -1)
            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
            attn = attn + relative_position_bias.unsqueeze(0)
        '''

        # TODO still using too much RAM
        #attn = self.softmax(attn)
        attn = nn.functional.softmax(attn, dim=-1)#, dtype=att_dtype)


        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        #print(f'Attention-  q:{q.size()}, k:{k.size()}, v:{v.size()}')
        #x = x.type(x_dtype)
        return x



def get_patches(x, patch_size):
    B, H, W, C = x.size()
    nh = H/patch_size
    nw = W/patch_size

    down_req = (nh-int(nh)) + (nw-int(nw))
    if down_req>0:
        new_dims = [int(nh)*patch_size, int(nw)*patch_size]
        x = downsampler_fn(x.permute(0, 3, 1, 2), new_dims).permute(0, 2, 3, 1)
        B, H, W, C = x.size()

    x = x.view(B,
                H // patch_size, patch_size,
                W // patch_size, patch_size,
                C
    )

    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, patch_size, patch_size, C)
    return windows, H, W


def get_image(windows, patch_size, Hatt, Watt, H, W):
    B = int(windows.size()[0] / (Hatt * Watt  / patch_size / patch_size ))
    x = windows.view(B, Hatt // patch_size,
                        Watt // patch_size,
                        patch_size, patch_size,
                        -1
    )
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hatt, Watt, -1)

    if H-Hatt + W-Watt :
        x = downsampler_fn(x.permute(0, 3, 1, 2), [H, W]).permute(0, 2, 3, 1)
    return x

class ViTBlock(nn.Module):
    def __init__(self,
                embedd_dim,
                input_dims,
                num_heads,
                MLP_type,
                patch_size,
                mlp_ratio,
                qkv_bias,
                qk_scale,
                drop,
                attn_drop,
                drop_path,
                act_layer,
                attention_type,
                norm_layer,
                layer_scale,
        )->None:
        super().__init__()
        self.patch_size = patch_size
        self.input_dims = input_dims
        #self.new_dims = [patch_size* (d//patch_size) for d in input_dims]
        #self.num_windows = prod_func([d//patch_size for d in self.new_dims])
        self.num_windows = prod_func([d//patch_size for d in input_dims])

        self.norm1 = norm_layer(embedd_dim)
        self.spatialConv = nn.Sequential(*[
                Rearrange("b h w c -> b c h w"),
                nn.Conv2d(embedd_dim, embedd_dim, groups=embedd_dim, kernel_size=3, padding=1,
                                 bias=False),
                get_norm('instance', num_features=embedd_dim),
                get_activation(act_layer),
                Rearrange("b c h w -> b h w c"),
        ])

        self.attn = Attention(embedd_dim,
                              attention_type=attention_type,
                              num_heads=num_heads,
                              patch_size=patch_size,
                              qkv_bias=qkv_bias,
                              qk_scale=qk_scale,
                              attn_drop=attn_drop,
                              proj_drop=drop,
        )

        self.drop_path = timm_DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(embedd_dim)
        self.mlp = MLP(in_feats=embedd_dim, hid_feats=int(embedd_dim * mlp_ratio),
                        act_name=act_layer, drop=drop,
                        MLP_type=MLP_type
        )

        self.layer_scale = False
        if layer_scale is not None and type(layer_scale) in [int, float]:
            self.layer_scale = True
            self.gamma1 = nn.Parameter(layer_scale * torch.ones(embedd_dim), requires_grad=True)
            self.gamma2 = nn.Parameter(layer_scale * torch.ones(embedd_dim), requires_grad=True)
        else:
            self.gamma1 = 1.0
            self.gamma2 = 1.0

    def forward(self, x):
        #x = downsampler_fn(x.permute(0, 4, 1, 2, 3), self.new_dims).permute(0, 2, 3, 4, 1)
        x = x.permute(0,2,3,1)

        B, H, W, C = x.size()

        ##print(B, H, W, C, ' x.shape')
        ##print(self.input_dims[0], self.input_dims[1], ' __init__ size \n')

        ##assert H == int(self.input_dims[0]) and W == int(self.input_dims[1]), 'declared data size is different from input x!'
        shortcut = x

        # TODO Mori v5
        #x =  self.spatialConv(x)

        x = self.norm1(x)
        x_windows, Hatt, Watt = get_patches(x, self.patch_size)
        x_windows = x_windows.view(-1, self.patch_size ** ndims, C)
        attn_windows = self.attn(x_windows)
        x = get_image(attn_windows, self.patch_size, Hatt, Watt, H, W)
        x = shortcut + self.drop_path(self.gamma1 * x)
        x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))
        return x.permute(0,3,1,2)

"""
if False:
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    hwd = [65, 15]
    embedd_dim = 64
    num_heads = 4
    patch_size = 6
    B = 1
    model = ViTBlock(embedd_dim=embedd_dim,
                    input_dims=hwd,
                    num_heads=num_heads,
                    MLP_type='basic',
                    patch_size=patch_size,
                    mlp_ratio=2,
                    qkv_bias=False,
                    qk_scale=None,
                    drop=0.,
                    attn_drop=0.,
                    drop_path=0.0,
                    act_layer='relu',
                    attention_type='global' ,
                    norm_layer=nn.LayerNorm,
                    layer_scale=None,
        ).to(device)

    xin = torch.normal(0, 1, (B,*hwd, embedd_dim)).to(device)
    if True: #with torch.cuda.amp.autocast():
        xout = model(xin)
        #max_mem_mb = torch.cuda.max_memory_allocated() / 1024.0 / 1024.0

        print(xin.size(), xin.dtype, 'xin')
        print(xout.size(), xout.dtype, 'xout')
    print('done!')
"""

    
def edge_extractor(yhat, levels=4):

    """  """
    def atw_kernel(ker_base, image_dtype, Cin, level=1):

        zeros_len = -1 + 2**(level-1)
        ker_len = zeros_len * 4 + len(ker_base)
        kernel_1d = np.zeros((ker_len,))
        kernel_1d[::zeros_len+1] = ker_base/np.sum(ker_base)
        kernel_2d = np.tensordot(kernel_1d, np.transpose(kernel_1d), axes=0)

        # convert To tensor
        kernel_size = kernel_2d.shape[0]
        kernel_torch = torch.tensor(kernel_2d, dtype=image_dtype).unsqueeze(0).expand(Cin, 1, kernel_size, kernel_size)
        return  kernel_torch, kernel_size

    def _convolve(image, ker_base, level):
        # get filter
        Cin = image.size(1)
        kernel_torch, pad_sz = atw_kernel(ker_base, image.dtype, Cin, level)

        # apply convolution
        output = F.conv2d(image, kernel_torch.to(device), stride=1, padding=int(pad_sz/2), groups=Cin)
        ##output = F.conv2d(image, kernel_torch, stride=1, padding=int(pad_sz/2), groups=Cin)
        return output

    B = yhat.size()[0]
    Cin = yhat.size(1)
    per_batch_loss = torch.Tensor([0.])
    ker_base = [0.002566, 0.1655, 0.6638, 0.1655, 0.002566]

    D = torch.zeros((yhat.size()), dtype=yhat.dtype).to(device)

    y_blur = _convolve(yhat, ker_base, 1)
    A = y_blur

    ker_base = [1., 4., 6., 4., 1.]
    for i in range(1, levels+1):
        y_blur_cur = _convolve(y_blur, ker_base, i)

        # get detail
        Di_y = y_blur - y_blur_cur

        # update x, y
        y_blur = y_blur_cur

        A = torch.cat([A,y_blur], dim=1)
        if i == 0:
            D = Di_y
        else:
            D = torch.cat([D, Di_y], dim=1)

    return A, D[:,Cin:,:,:]


# Attention Branch
class AttentionBranch(nn.Module):
    def __init__(self, n_feat, k_size=3):

        super(AttentionBranch, self).__init__()
        self.k1 = nn.Conv2d(n_feat, n_feat, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) # 3x3 convolution
        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)
        self.k2 = nn.Conv2d(n_feat, n_feat, 1) # 1x1 convolution nf->nf
        self.sigmoid = nn.Sigmoid()
        self.k3 = nn.Conv2d(n_feat, n_feat, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) # 3x3 convolution
        self.k4 = nn.Conv2d(n_feat, n_feat, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) # 3x3 convolution

    def forward(self, x):

        y = self.k1(x)
        y = self.lrelu(y)
        y = self.k2(y)
        y = self.sigmoid(y)

        out = torch.mul(self.k3(x), y)
        out = self.k4(out)

        return out

class Residual_block(nn.Module):
  def __init__(self, in_ch=64, out_ch=64, stride=1, bias=False):
    super(Residual_block,self).__init__()
    self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding =1, bias=bias)
    self.relu1 = nn.ReLU(inplace=False)
    self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding =(1,1), bias=bias)

  def forward(self,inp):
    x = self.conv1(inp)
    x = self.relu1(x)
    x = self.conv2(x)
    return x + inp


def conv(in_ch=64, out_ch=64, stride=1, bias=False):
  return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding =1, bias=bias)



class BasicBlock(nn.Module):
  def __init__(self, in_ch=3,
               out_ch=3,
               n_feat=64,
               n_blocks_stage1=5,
               n_blocks_stage2=5,
               stride=1,
               bias=False,
               levels=3,

               # ViT params
               num_heads=2,
               patch_size=8,
               mlp_ratio=2,
               input_dims=[341, 341]
    ):
    super(BasicBlock, self).__init__()

    ##print("input_dims:", input_dims)
    ##exit()

    self.levels= levels
    self.relu = nn.ReLU(inplace=False)

    """ stage (I) """
    self.conv_1 = conv(in_ch=n_feat, out_ch=n_feat, stride=1, bias=False)
    self.conv_2 = conv(in_ch=n_feat, out_ch=out_ch, stride=1, bias=False)

    self.att = ViTBlock(embedd_dim=n_feat,
                    input_dims=input_dims,
                    num_heads=num_heads,
                    MLP_type='basic',
                    patch_size=patch_size,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=False,
                    qk_scale=None,
                    drop=0.,
                    attn_drop=0.,
                    drop_path=0.0,
                    act_layer='relu',
                    attention_type='global',
                    norm_layer=nn.LayerNorm,
                    layer_scale=None,
        )

    self.conv_3 = conv(in_ch=2*in_ch*levels, out_ch=n_feat, stride=1, bias=False)
    self.conv_4 = conv(in_ch=n_feat, out_ch=n_feat, stride=1, bias=False)
    self.conv_5 = conv(in_ch=n_feat, out_ch=out_ch, stride=1, bias=False)


    self.stageI = nn.Sequential(*[Residual_block(in_ch=n_feat, out_ch=n_feat) for _ in range(n_blocks_stage1)])

    """ stage (II) """
    self.stageII_coarse = nn.Sequential(*[Residual_block(in_ch=n_feat, out_ch=n_feat) for _ in range(n_blocks_stage2)])
    self.stageII_fine = nn.Sequential(*[Residual_block(in_ch=n_feat, out_ch=n_feat) for _ in range(n_blocks_stage2)])
    self.stageII_att = nn.Sequential(*[AttentionBranch(n_feat) for _ in range(n_blocks_stage2)])


  def forward(self,x,atw_img):
    """ stage (I) """
    x = self.stageI(x)

    # img_r1
    img_r = self.conv_1(x)
    img_r = self.relu(img_r)
    #img_r = self.att(img_r.permute(0,2,3,1)).permute(0,3,1,2)
    img_r = self.conv_2(img_r)

    """stage (II)"""
    # features
    _, atw_r = edge_extractor(img_r, levels=self.levels)
    atw = torch.cat([atw_img,atw_r], axis=1)
    atw = self.conv_3(atw)

    for i, (c,f,a) in enumerate(zip(self.stageII_coarse,self.stageII_fine,self.stageII_att)):
      c_i = c(x)
      atw = f(atw)
      a_i = a(atw)
      x = c_i + torch.mul(a_i,c_i)

    x = self.att(x)
    x = self.conv_4(x)
    
    # img_r2
    img_r = self.conv_5(x)

    return x, img_r






class UpScale(nn.Module):
    def __init__(self, in_ch, upscale_x, upscale_y):
        super(UpScale, self).__init__()

        self.upscale_x = upscale_x
        self.upscale_y = upscale_y
        upscale_ratio = max(int(upscale_x),int(upscale_y))


        self.conv_1 = conv(in_ch= in_ch, out_ch= in_ch * (upscale_ratio **2), stride=1, bias=False)
        self.conv_2 = conv(in_ch=  in_ch * (upscale_ratio **2), out_ch= in_ch * (upscale_ratio **2), stride=1, bias=False)
        self.conv_3 = conv(in_ch=  in_ch * (upscale_ratio **2), out_ch= in_ch * upscale_ratio, stride=1, bias=False)
        self.conv_4 = conv(in_ch=  in_ch * upscale_ratio, out_ch= in_ch , stride=1, bias=False)


    def forward(self,x):


        shape_Xup_x = int(x.shape[-2]*self.upscale_x)
        shape_Xup_y = int(x.shape[-1]*self.upscale_y)
        shape_up_xy = (shape_Xup_x, shape_Xup_y)

        X_up = F.interpolate(x, size = shape_up_xy , mode='bilinear',align_corners=False)#.clamp(min=0, max=1.0)

        X_up = self.conv_1(X_up)
        X_up = self.conv_2(X_up)
        X_up = self.conv_3(X_up)
        X_up = self.conv_4(X_up)

        return X_up


def upsample_shuffle(in_ch, upscale=3):
    if upscale ==2 or upscale==3:
        return nn.Sequential(nn.Conv2d(in_ch, out_channels=(upscale**2) * in_ch, kernel_size=3, stride=1, padding=1, bias=False),
                            nn.PixelShuffle(upscale),
                            )

    if upscale ==4:
        return nn.Sequential(nn.Conv2d(in_ch, out_channels=(upscale*2) * in_ch, kernel_size=3, stride=1, padding=1, bias=False),
                            nn.PixelShuffle(2),
                            nn.Conv2d(in_ch, out_channels=(upscale*2) * in_ch, kernel_size=3, stride=1, padding=1, bias=False),
                            nn.PixelShuffle(2),
                            )

def upsample_interpolate(img, sz):
    return F.interpolate(img, size = sz, mode='bilinear',align_corners=False)#.clamp(min=0, max=1.0)



class SRNET(nn.Module):
  def __init__(self, in_ch=3, out_ch=3, n_feat=64, n_blocks=3, stride=1, bias=False,
               levels=3, upscale_ratio_x=3, upscale_ratio_y=3,
               input_size=(682,341), #   self.upscale_sz = tuple([ int(dim*upscale_ratio_y) for dim in input_size])  <<<<<<<<<<<<<<<<<<<
               num_heads=16,
               patch_size=4,
               mlp_ratio=2,):
    super(SRNET, self).__init__()

    self.upscale_sz = tuple([ int(dim*upscale_ratio_y) for dim in input_size])
    self.att_inpSZ =  [ int(dim*upscale_ratio_y) for dim in input_size]


    upscale = max(int(upscale_ratio_x),int(upscale_ratio_y))
    min_upscale = min(int(upscale_ratio_x),int(upscale_ratio_y))  # for first level of upscale using shuffling, integer.

    self.levels= levels

    self.conv_1 = conv(in_ch=in_ch, out_ch=n_feat, stride=1, bias=False)

    self.relu = nn.ReLU(inplace=False)

    self.BasicBlocks = nn.Sequential(*[ BasicBlock(in_ch=in_ch,
                                                   out_ch=out_ch,
                                                   n_feat=n_feat,
                                                   n_blocks_stage1=4,
                                                   n_blocks_stage2=2,
                                                   input_dims=input_size,
                                                   num_heads=num_heads,
                                                   patch_size=patch_size,
                                                   mlp_ratio=mlp_ratio,
                                                   stride=1, bias=False,
                                                   levels=levels) for _ in range(n_blocks)]) # block_num = n_blocks




    self.conv_2 = conv(in_ch=n_feat, out_ch=out_ch, stride=1, bias=False)

    # self.conv_3 = nn.Sequential(conv(in_ch= n_feat, out_ch=(upscale**2)*n_feat, stride=1, bias=False),
    #                             conv(in_ch= (upscale**2)*n_feat , out_ch=(upscale**2//2)*n_feat, stride=1, bias=False),
    #                             conv(in_ch= (upscale**2//2)*n_feat , out_ch= n_feat, stride=1, bias=False),
    #                             )

    self.conv_4 = conv(in_ch=n_feat, out_ch=out_ch, stride=1, bias=False)

    #self.BasicBlocks_up = nn.Sequential(*[ BasicBlock(in_ch=in_ch, out_ch=out_ch, n_feat=n_feat, n_blocks_stage1=4, n_blocks_stage2=2, stride=1, bias=False, levels=levels) for _ in range(1)]) # block_num = 1

    self.conv_before_upsample = nn.Sequential(nn.Conv2d(n_feat, n_feat, 3, 1, 1),  nn.LeakyReLU(inplace=True))


    self.upsample_shuffle = upsample_shuffle(n_feat, min_upscale)


    #self.condconv1 = CondConv2D(n_feat, n_feat, kernel_size=3, num_experts=4, dropout_rate=0.0)

    #self.condconv2 = CondConv2D(n_feat, n_feat, kernel_size=3, num_experts=4, dropout_rate=0.0)

    #self.LRelu= nn.LeakyReLU(inplace=True)

    # self.att_up = ViTBlock(embedd_dim=n_feat,
    #             input_dims= self.att_inpSZ ,
    #             num_heads=num_heads,
    #             MLP_type='basic',
    #             patch_size=patch_size*2,
    #             mlp_ratio=mlp_ratio,
    #             qkv_bias=False,
    #             qk_scale=None,
    #             drop=0.,
    #             attn_drop=0.,
    #             drop_path=0.0,
    #             act_layer='relu',
    #             attention_type='global' ,
    #             norm_layer=nn.LayerNorm,
    #             layer_scale=None,
    # )




  def forward(self, img):
    #Input
    x = self.conv_1(img)
    long_skip = x

    _, atw_img = edge_extractor(img, levels=self.levels)

    #BasicBlocks
    for i,b in enumerate(self.BasicBlocks):
        x, _ = b(x,atw_img)
        if i==2:
            x_downscale = self.conv_2(x)

    #UpScale
    # upscale_sz = ( int(img.shape[-2]*self.upscale_x), int(img.shape[-1]*self.upscale_y) ) # desired size for super-resolved images, non-integer.

    if img.shape[-2] == img.shape[-1]:
        upscale_sz = (min(self.upscale_sz[-2],self.upscale_sz[-1]),min(self.upscale_sz[-2],self.upscale_sz[-1]))
    else:
        upscale_sz = self.upscale_sz

    img_up = upsample_interpolate(img, upscale_sz)
    _, atw_img_up = edge_extractor(img_up, levels=self.levels)

    #long skip
    x = x + long_skip

    x = self.upsample_shuffle(x)

    # apply attention in output
    #x = self.att_up(x)

    #print(x.shape)
    #x = self.condconv1(x)
    #x = upsample_interpolate(x,upscale_sz)
    #x = self.condconv2(x)

    #UpScale blocks
    #for b in self.BasicBlocks_up:
    #    x, _ = b(x,atw_img_up)

    #output
    #x = self.conv_3(x)
    #x = self.LRelu(x)
    x = self.conv_4(x)
    #x = self.relu(x)
    return x  # x_downscale, x







